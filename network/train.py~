#
# File to test the hnn.py class. 
#
# This file establishes the clebsch gordan coefficients, sets up an hnn with given parameters,
# loads holograms from .npy files, and then tests the network via a function call.
#


import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))


import numpy as np
import linearity
import nonlinearity
import hnn
import os

L_MAX = 6

# load clebsch gordan coefficients
cg_file = 'CG_matrix_l=10.npy'
cg_matrices = np.load(cg_file, allow_pickle=True).item()

tf_cg_matrices = {}
for l in range(L_MAX + 1):
    for l1 in range(L_MAX + 1):
        for l2 in range(0,l1+1):
            tf_cg_matrices[(l,l1,l2)] = tf.convert_to_tensor(cg_matrices[(l,l1,l2)],dtype=tf.complex64)
                    

# network parameters
num_layers = 5
num_aa = 20
hidden_l_dims = []
for i in range(num_layers):
    curr_l_dims = []
    for l in range(L_MAX + 1):
        curr_l_dims.append(10)
    hidden_l_dims.append(curr_l_dims)
print('Making network with L_MAX=' + str(L_MAX) + ' with '  + str(num_layers) + ' layers with hidden dimensions ' + 
      str(hidden_l_dims))

network = hnn.hnn(L_MAX, hidden_l_dims, num_layers, num_aa, tf_cg_matrices)

@tf.function
def loss_fn(truth, pred):
    return tf.nn.softmax_cross_entropy_with_logits(
        labels = truth,
        logits = pred)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

network.compile(optimizer=optimizer, loss=loss_fn, metrics =['categorical_accuracy'])

# load premade holograms
k = 0.0001
d = 10.0
examples_per_aa = 1000
examples_per_aa_val = 100
d_val = 10.0

print('Loading test input to model')
train_hgrams_real = np.load('train_hgram_real_example_examplesPerAA=' + str(examples_per_aa) + '_k=' + str(k) + '_d=' + str(d) + '_l=' + str(L_MAX) + '.npy'
                            ,allow_pickle=True,encoding='latin1')[()]
train_hgrams_imag = np.load('train_hgram_imag_example_examplesPerAA=' + str(examples_per_aa) + '_k=' + str(k) + '_d=' + str(d) + '_l=' + str(L_MAX) + '.npy'
                            ,allow_pickle=True,encoding='latin1')[()]
train_hgrams = {}
for l in range(L_MAX + 1):
    train_hgrams[l] = (train_hgrams_real[l] + 1j * train_hgrams_imag[l]).astype("complex64")

print('Loading vaalidation input to model')
val_hgrams_real = np.load('train_hgram_real_example_examplesPerAA=' + str(examples_per_aa_val) + '_k=' + str(k) + '_d=' + str(d_val) + '_l=' + str(L_MAX) + '.npy'
                            ,allow_pickle=True,encoding='latin1')[()]
val_hgrams_imag = np.load('train_hgram_imag_example_examplesPerAA=' + str(examples_per_aa_val) + '_k=' + str(k) + '_d=' + str(d_val) + '_l=' + str(L_MAX) + '.npy'
                            ,allow_pickle=True,encoding='latin1')[()]
val_hgrams = {}
for l in range(L_MAX + 1):
    val_hgrams[l] = (val_hgrams_real[l] + 1j * val_hgrams_imag[l]).astype("complex64")


labels = np.load('train_labels_examplesPerAA=' + str(examples_per_aa) + '_k=' + str(k) + '_d=' + str(d) + '_l=' + str(L_MAX) + '.npy',
                 allow_pickle=True,encoding='latin1')



val_labels = np.load('train_labels_examplesPerAA=' + str(examples_per_aa_val) + '_k=' + str(k) + '_d=' + str(d_val) + '_l=' + str(L_MAX) + '.npy',                 allow_pickle=True,encoding='latin1')


print('Running network via predict')
#print(network.predict(train_hgrams,batch_size=32))
pred_i = network.predict(train_hgrams,batch_size=2)
print(network.summary())

#print(network.weights)

#initial_weights = network.weights
#i_weights = network.get_weights()

print('Training network')
network.fit(x=train_hgrams,y=labels,batch_size=2,epochs=5,shuffle=True,
            validation_data=(val_hgrams,val_labels))
network.save_weights('./saved_weights/weights')
from keras import backend as K
K.set_value(network.optimizer.learning_rate, 0.0001)
network.fit(x=train_hgrams,y=labels,batch_size=1,epochs=5,shuffle=True,
            validation_data=(val_hgrams,val_labels))
network.fit(x=train_hgrams,y=labels,batch_size=4,epochs=10,shuffle=True,
            validation_data=(val_hgrams,val_labels))

#pred_f = network.predict(train_hgrams,batch_size=10)
#final_weights = network.weights
#f_weights = network.get_weights()


#print('Initial weights = ' +str(i_weights))
#print('Final weights = ' +str(f_weights))


#for i in range(len(network.weights)):
#    print('Difference in weights: ')
#    print(final_weights[i] - initial_weights[i])
#    print('\n')

#print('Difference in prediction:')
#print(pred_f - pred_i)
#print('\n')
print('Terminating successfully')
