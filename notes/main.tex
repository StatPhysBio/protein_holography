
\documentclass{article}
% Packages
\usepackage[a5paper, textwidth=130mm, textheight=200mm]{geometry}
\usepackage{amsmath}
\usepackage{unicode-math}
\usepackage{bm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{braket}
\usepackage{scalerel,stackengine}
\usepackage{amssymb}

\title{Holographic Learning of Protein Structures}
\author{Michael Pun}
\date{21 May 2020}

\begin{document}

\maketitle

\tableofcontents

%
% PROTEIN SECTION
%
\section{Protein background}
\subsection{Functions}

\subsection{Structure}

\subsubsection{Secondary Structure}

\includegraphics[width=8cm]{Ramachandran_angle_classification.png}

\subsection{Attempts at learning proteins}


%
% HOLOGRAPHY SECTION
%
\section{Holography}

\subsection{In Line Holography}
    \begin{center}
        \includegraphics[width=8cm]{Gabor holography.png}
    \end{center}
    Typical holography involves shiniing a coherent monochromatic light source on an object and recording in intensity of an interference pattern that the re-emitted light makes with a reference beam. In Figure 1, the object is semi-transparent so the reference wave is the source wave that is not absorbed by the object.\cite{Gabor Holography}\\
    
    The holographic film is assumed to have a transmission function that is proportional to the intensity of the light recieved $t(x,y) = a + bI(x,y)$ where $I(x,y) = |U_r + U_o(x,y)|^2$. \\
    
    This means when the film is illuminated with another coherent monochromatic beam $U_0$, the light transmitted is given by 
    \begin{align}
        U(x,y) &= U_0 t(x,y)\\
        &= U_0(a+b|U_0|^2) + bU_0|U_o(x,y)|^2 + b|U_0|^2U_o(x,y) + b U_0^2 U_o^*(x,y)
    \end{align}
    
    The last two terms contain the field information due to the object and by uniqueness of Maxwell's equations, they imply two images of the object--one virtual and one real.
    
    The takeaway for our purposes is that the reference beam is used simply to create an interference pattern that can then recover the full complex information of the original field. 
    
    If we can simply write down the complex field on the recording surface then we are effectively doing holography.
    
\subsection{Spherical holography}

The idea of spherical holography is to use the holographic framework to project the information contained in 3-dimensional point cloud onto a 2-dimensional surface in a rotationally covariant way. 

The use of planar waves and a planar recording surface breaks spherical symmetry. In spherical symmetry, we use a spherical reference wave and a spherical recording surface.

We can avoid choosing a preferred direction; however we must choose an origin for our spherical waves and recording surface. Once this point is chosen, a spherical wave is emmitted from the origin and travels outward exciting the points in our point cloud individually. These excited points then re-emit a spherical wave with the same frequency. 

We assume enough time has passed so all points have been radiating for long enough so that the wavefronts from all points have completely passed the spherical recording surface. The complete complex electric field is recorded at this time ignoring the reference wave. This recording is our spherical hologram.

\begin{center}
\includegraphics[width=8cm]{hologram_example}\\
Figure: Illustration to show the mechanics of spherical holography. 
\end{center}
Spherical wave expansion 
\begin{align}
    \frac{e^{ik|\textbf{r}-\textbf{r}'|}}{|\textbf{r}-\textbf{r}'|} = ik\sum_lj_l(kr_<)h_l^{(1)}(kr_>)\sum_m Y^*_{lm}(\theta',\phi')Y_{lm}(\theta,\phi).
\end{align}


%
% GROUP THEORY SECTION
%
\section{Group Theory}


\subsection{Wigner D-matrices are irreps of SO(3)}
We know from study of SO(2) that the rotations about an axis $\hat{n}$ are generated by $J_n$. In other words, $$R_n(\theta) = e^{-i\theta J_n} .$$

Meanwhile we can parametrize rotations in SO(3) by the Euler angles and express elements of SO(3) as 
\begin{align}
    R(\alpha,\beta,\gamma) &= R_3(\alpha)R_2(\beta)R_3(\gamma)\\
    &= e^{-i\alpha J_3} e^{-i\beta J_2} e^{-i\gamma J_3}
\end{align}

Now if we can find a basis which is simultaneous eigenstates we would already have found our representation. Unfortunately $J_2$ and $J_3$ do not commute. Thus we need to look for a different basis. 

The operator $J^2$ commutes with all generators of SO(3). We will eventually use a simultaneous basis of $J^2$ and $J_3$ as our basis for the irreducible representations. 

Since $J^2$ commutes with all generators of our group algebra, Schur's first lemma tells us that $J^2$ is a multiple of the identity in any irreducible representation. 

Meanwhile we note that the commutator of any two generators is \begin{align}
    [J_i,J_j] = i \epsilon_{ijk}J_k
\end{align}
This statement is equivalent to the group multiplication rule.

We also define $$J_{\pm} = J_1 \pm iJ_2$$ which will be a raising (lowering) operator. Using commutation relations and identities we see that eigenvectors of $J_3$ must have eigenvalues given by $m = -j,-j+1,...,j-1,j$ where $j$ is related to eigenvalue of $J^2$. Knowing that the raising and lowering operators raise and lower the eigenvalue of $J_3$ by 1 at each application, we get that $j = 0,1/2,1,3/2,...$ 
This allows us to use the basis $\ket{jm}$ to see our irreducible representations are labeled by $j$.

Specifically these basis vectors transform under rotations as 
\begin{align}
    U(\alpha,\beta,\gamma)\ket{jm} = \ket{jm'}D^j(\alpha,\beta,\gamma)^{m'}_m
\end{align}
where the Wigner D-matrices are given by 
\begin{align}
    D^j(\alpha,\beta,\gamma)^{m'}_m = e^{-i\alpha m'}d^j(\beta)^{m'}_m e^{-i\gamma m'}
\end{align}
where
\begin{align}
    d^j(\beta)^{m'}_m = \bra{jm'}e^{-i\beta J_2}\ket{jm}
\end{align}

\subsection{Wigner D-matrices form complete orthogonal basis over functions from SO(3) $\rightarrow \mathbb{C}$}

Orthonormality:
\begin{align}
    (2j+1)\int d\tau_R D^{\dagger}_j(R)^m_n D^{j'}(R)^{n'}_{m'} = \delta^{j'}_j\delta^{n'}_n\delta^{m}_{m'}
\end{align}
Completeness:
\begin{align}
    \sum_{jmn}(2j+1)D^{\dagger}_j(R')^m_n D^{j}(R)^{n}_{m} = \delta(R - R')
\end{align}

\subsection{Wigner D-matrices naturally decompose $\mathbb{C}(SO(3))$ into invariant subspaces via the projection operators}

Suppose we have a state of fixed "linear momentum" $\ket{p}$ and with spin $\lambda$


\subsubsection{Mike's convolution}
Given two functions $f,h:SO(3)\rightarrow \mathbb{C}$, the convolution is 
\begin{align}
    (h\star f)(R) = \int dS f^{*}(S)h(S^{-1}R)
\end{align}
The fourier coefficient of a convolution is
\begin{align}
    (h \star f)^{l}_{mm'} &= \int dR D^{\dagger}_l(R)_{mm'} \int dS f^{*}(S)h(S^{-1}R)\\
    &= \int dS D^{\dagger}_l(S)_{mn} f^{*}(S) \int dR D^{\dagger}_l(S^{-1}R)_{nm'} g(S^{-1}R)\\
    &= f^{l}_{mn}g^l_{nm'}
\end{align}
\subsubsection{Convolution for functions  over $S^2$}
Let $f,h:S^2\rightarrow \mathbb{C}$. The convolution is then
\begin{align}
    (h\star f)(R) &= \int d\Omega f(\hat{n})h(R^{-1}\hat{n})\\
    (h\star f)^{l}_{mn} &= \int dR D^{\dagger}_{l}(R)_{mn}\int d\Omega \sum_{l'm'}f^{*}_{l'm'} Y^{*}_{l'm'}(\hat{n}) \sum_{l''m''}h_{l''m''}D_{l''}(R)_{n''m''}Y_{l''n''}(\hat{n})\\
    &= \int d\Omega \sum_{l'm'}\sum_{l''m''} f^{*}_{l'm'}h_{l''m''}Y^{*}_{l'm'}(\hat{n})Y_{l''n''}(\hat{n})\underbrace{\int dR D^{\dagger}_l(R)_{mn}D_{l''}(R)_{n''m''}}_{(2l+1)^{-1}\delta_{ll''}\delta_{mn''}\delta_{nm''}}\\
    &=  (2l+1)^{-1}\sum_{l'm'} f^{*}_{l'm'}h_{ln} \underbrace{\int d\Omega Y^{*}_{l'm'}(\hat{n})Y_{lm}(\hat{n})}_{\delta_{l'l}\delta_{m'm}}\\
    &= (2l+1)^{-1} f^{*}_{lm}h_{ln}\\ 
\end{align}

\section{Covariant network}
The covariant network is the Clebsch-Gordan Network presented in \cite{CG Net}. The network takes in information classified by representation of SO(3). It then performs two operations within one layer of the network. First, a linear combination is performed where only information of corresponding to the same representation is combined. Second, a non-linearity is taken between all representation coefficients. While this non-linearity is generally a linear combination of multiple representations, it can be decomposed into a sum of representations using the Clebsch-Gordan coefficients.

The following are the details of how my network is currently running.


\subsection{Inputs}
The input to the network is a collection of fourier coefficients $f^0_{lc^l_0m}$ where $l$ indexes the representation and $c$ indexes the channel. The index $m$ is a function of $l$ and can take on $2l+1$ values.

\subsection{Linear Combination}
The first step to transform layer zero into layer one is to take a linear combination of the inputs. In general, this linear combination can either be a dimensional reduction or expansion. 

Let $W^i_{c_{i}c^l_{i-1}}$ be a set of learnable weights. The parameter $c_i$ is the dimension we want to expand or reduce the input into. The linear combination is then
\begin{align}
    g^{i}_{lc_{i}m} = W^i_{c_{i}c^l_{i-1}}f^{i-1}_{lc^l_{i-1}m} 
\end{align}

\subsection{Non-linearity & Clebsch-Gordan Decomposition}
The next step from layer to layer is taking a non-linearity of the linear combination. This non-linearity can be decomposed with Clebsch-Gordan coefficients which can be grouped into two dimensional matrices $C^l_{l_1 l_2}$ of size $(2l_1+1)\times(2l_2+1)$.

The non-linear product is then
\begin{align}
    f^{i}_{lc^l_im} = \big(C^l_{l_1l_2}\big)_{m_1m_2} g^{i}_{l_1c_{i}m_1} g^{i}_{l_2c_{i}m_2}
\end{align}

\subsection{Fully Connected Layer}
For classification tasks, the scalar information of the last layer is used as the input into a fully connected layer. For an $n$ layer network with $p$ classes, this operation looks like
\begin{align}
    f^{n+1}_k = W^{n+1}_{kc^0_{n-1}} f^{n}_{0c^0_{n}}
\end{align}
where $k$ takes on $p$ possible values. $f^{n+1}_k$ are the log probabilities of each of the $p$ classes.

\section{Results}

\subsection{Classification of Amino Acid Structures}

\includegraphics[width=8cm]{accVsR.png}\\
\includegraphics[width=8cm]{accVsNoise.png}

\subsection{Classification of neighborhoods}

\subsubsection{Classification of neighborhoods restricted to secondary structure}

\begin{thebibliography}{}
\bibitem{Gabor Holography}
Gabriel Popescu. 
\textit{Holography}.
http://light.ece.illinois.edu/ECE460/PDF/Holography.pdf

\bibitem{CG Net}
Risi Kondor, Zhen Lin, Shubhendu Trivedi.
Clebsch-Gordan Nets: A Fully Fourier Space Spherical Convolution Neural Network
arXiv:1806.09231v2
\end{thebibliography}
\end{document}

